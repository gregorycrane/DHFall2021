{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uYfp9wZJRJo1"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "# check out spaCy documentation here: https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qJ5VylFaRnkv"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Mohammed, Sultan of Carizme, reigned in Sogdiana, when it was invaded (A.D. 1218) by Zingis and his Moguls.\")\n",
    "# doc = nlp(\"Mohammed, Sultan of Carizme, reigned in Sogdiana, when it was invaded (A.D. 1218) by Zingis and his Mongols.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JNQ--v-8Rnnm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mohammed', 'PERSON'), ('Sultan', 'PERSON'), ('Carizme', 'PERSON'), ('Sogdiana', 'GPE'), ('A.D. 1218', 'DATE'), ('Zingis', 'PERSON'), ('Moguls', 'NORP')]\n"
     ]
    }
   ],
   "source": [
    "# document level\n",
    "ents = [(e.text, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "\n",
    "\n",
    "# for more on label conventions for en_core_web_md, see: https://spacy.io/models/en#en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljMUuBkzRnqg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNlYu2NXR9E_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UokuzU5bU4EO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObOyiw_4U4HE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NfYHF9NRU4J-"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gibbonfortm.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-73ad0cd38911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gibbonfortm.xml'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgibbon_fh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgibbon_fh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gibbonfortm.xml'"
     ]
    }
   ],
   "source": [
    "with open('gibbonfortm.xml') as gibbon_fh:\n",
    "  soup = BeautifulSoup(gibbon_fh, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4kTAB1loOxk"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VqFKX-elg3v"
   },
   "outputs": [],
   "source": [
    "\n",
    "div_list = soup.findAll('div')\n",
    "\n",
    "\n",
    "chapter_list = []\n",
    "for div in div_list:\n",
    "  div_type = div.get('subtype')\n",
    "  if div_type == 'chapter':\n",
    "    chapter_list.append(div)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgLjRXQXsZC9"
   },
   "outputs": [],
   "source": [
    "chapter_number = ### YOUR CODE HERE ###\n",
    "\n",
    "for i in range(len(chapter_list)):\n",
    "  chap_num = chapter_list[i].get('n')\n",
    "  if chap_num == chapter_number:\n",
    "    text = chapter_list[i].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0CBcc15tea1"
   },
   "outputs": [],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBFpxT1_umY-"
   },
   "outputs": [],
   "source": [
    "full_text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccJTA6UHvJdT"
   },
   "outputs": [],
   "source": [
    "full_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VFoOW-alhBG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBA9s_JcU4NM"
   },
   "outputs": [],
   "source": [
    "# Create a new string called text_seg that only contains the first 10000 characters of the chapter you are working with\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aW6Oq4KJe7VP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqm82V-xL-bF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n694k1oLiaRM"
   },
   "outputs": [],
   "source": [
    "# Run an NLP analysis on the text segment, generate a list of the named entities in the text, and print out the list of named entities\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLVI5FaMoV5M"
   },
   "outputs": [],
   "source": [
    "# Populate the empty list below with only the named entities classed as 'GPE' or 'LOC'\n",
    "\n",
    "\n",
    "places = []\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yP91iwpZp-fu"
   },
   "outputs": [],
   "source": [
    "places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSjyBtuwqoZp"
   },
   "outputs": [],
   "source": [
    "place_names_duplicates = [x[0] for x in places]\n",
    "\n",
    "print(len(place_names_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWxoQRNCqAJ2"
   },
   "outputs": [],
   "source": [
    "place_names_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rc_rCff0qPmx"
   },
   "outputs": [],
   "source": [
    "# Populate the empty list below with the identified place names but remove any duplicates so each name occurs only once\n",
    "\n",
    "place_names = []\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sERMzuK_Nd-w"
   },
   "outputs": [],
   "source": [
    "place_counts = []\n",
    "\n",
    "for place in place_names:\n",
    "  place_counts.append(place_names_duplicates.count(place))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10id2A6vqPr9"
   },
   "outputs": [],
   "source": [
    "# sanity check!\n",
    "print(len(place_names), len(place_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d732qJouTA5A"
   },
   "outputs": [],
   "source": [
    "counts_dict = dict(zip(place_names, place_counts))\n",
    "# counts_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUoGayZT5OQN"
   },
   "source": [
    "### Pause for discussion!\n",
    "\n",
    "Why would we want to include the counts of mentions of places for mapping purposes?\n",
    "\n",
    "In your groups, identify another feature you might want to include on the map. Sketch out a process for extracting that information and including it as an additional column in the .csv file you use to generate the map. This can be in general terms or using some pseudo-code. Be prepared to share!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zb49KomeqPxU"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def search_peripleo(name, fuzzy=False, datasets=('pleiades'), from_date=-3000, to_date=2000, retry_attempts=10):\n",
    "    \"\"\"\n",
    "    get the raw response back from peripleo as a dictionary. look here for more details: https://github.com/pelagios/peripleo/blob/main/README.md\n",
    "    :param name: place name to search\n",
    "    :param fuzzy: whether a fuzzy search should be performed\n",
    "    :param datasets: which datasets should be included in the search\n",
    "    :param from_date: start date for search\n",
    "    :param to_date: end date for search\n",
    "    :param retry_attempts: how many times to retry the request if it fails\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # pause execution to prevent dos'ing the GeoNames server\n",
    "    time.sleep(0.3)\n",
    "\n",
    "    # default parameters\n",
    "    params = {\n",
    "        'query': name,\n",
    "        'types': 'place',\n",
    "        'from': from_date,\n",
    "        'to': to_date,\n",
    "        'datasets': datasets\n",
    "    }\n",
    "\n",
    "    if fuzzy:\n",
    "        params['query'] = params['query'] + '~'\n",
    "\n",
    "    gz_url = 'http://peripleo.pelagios.org/peripleo/search'  # baseurl for peripleo search\n",
    "\n",
    "    try:\n",
    "        response = requests.get(gz_url, params=params, timeout=None)\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        # if specified in the arguments, retry the API call on request timeout\n",
    "        print(e)\n",
    "        if retry_attempts > 0:\n",
    "            retry_attempts -= 1\n",
    "            return search_name(name, retry_attempts=retry_attempts)\n",
    "        else:\n",
    "            raise Exception('Timeout after specified retries.')\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    \n",
    "    # for any response where the status code is not 200 ('success') retry the API call\n",
    "    if retry_attempts > 0:\n",
    "        retry_attempts -= 1\n",
    "        return search_name(name, retry_attempts=retry_attempts)\n",
    "    else:\n",
    "        raise Exception('Status code: ' + str(response.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdLjqYH0qQc0"
   },
   "outputs": [],
   "source": [
    "# match all places nouns against the gazetteer\n",
    "peripleo_results = []\n",
    "num_places = len(place_names)\n",
    "prev_disp_percent = 0\n",
    "for i in range(len(place_names)):\n",
    "    place = place_names[i]\n",
    "    print(f'matching {place} against peripleo (Pleiades) gazetteer...')\n",
    "    # search has default time bounds and default dataset is Pleiades. look at the code for details\n",
    "    try:\n",
    "        peripleo_results.append({'token': place, 'results': search_peripleo(place, fuzzy=False)})\n",
    "    except NameError:\n",
    "        print('NameError', place)\n",
    "        continue\n",
    "    percent_done = (i / num_places) * 100\n",
    "    disp_percent = percent_done // 1\n",
    "    if disp_percent > prev_disp_percent:\n",
    "        print('\\n' + str(disp_percent) + '% of identified places have been checked against gazetteer...\\n')\n",
    "        prev_disp_percent += 1\n",
    "    \n",
    "print('complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qme2vnFtCOtK"
   },
   "outputs": [],
   "source": [
    "len(peripleo_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11BDWuwlsQf3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPtv7v-urM2X"
   },
   "outputs": [],
   "source": [
    "# simple georesolution where the first result from the gazetteer is taken as the value\n",
    "peripleo_places = [{'token': i['token'], 'place': i['results']['items'][0]} for i in peripleo_results if len(i['results']['items']) > 0]\n",
    "peripleo_places[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_hsTxpmR1Zf"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVu7paWuq8rV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "tokens = []\n",
    "names = []\n",
    "longs = []\n",
    "lats = []\n",
    "identifiers = []\n",
    "counts = []\n",
    "\n",
    "\n",
    "for p in peripleo_places:\n",
    "    token = p['token']\n",
    "    name_s = p['place']['names']\n",
    "#     average the minimum and maximum longitudes and latitutdes\n",
    "    try:\n",
    "        long = ((p['place']['geo_bounds']['max_lon']) + (p['place']['geo_bounds']['min_lon'])) / 2\n",
    "        lat = ((p['place']['geo_bounds']['max_lat']) + (p['place']['geo_bounds']['min_lat'])) / 2\n",
    "        count = counts_dict[token]\n",
    "    except KeyError:\n",
    "        long = '?'\n",
    "        lat = '?'\n",
    "    identifier = p['place']['identifier']\n",
    "    tokens.append(token)\n",
    "    names.append(name_s)\n",
    "    longs.append(long)\n",
    "    lats.append(lat)\n",
    "    identifiers.append(identifier)\n",
    "    counts.append(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwLsr9GnrEl1"
   },
   "outputs": [],
   "source": [
    "place_df = pd.DataFrame({'token': tokens, 'names': names, \"latitude\": lats, 'longitude': longs, \"identifier\": identifiers, \"counts\": counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZ6W1g2TrUEQ"
   },
   "outputs": [],
   "source": [
    "# export pandas dataframe to csv file\n",
    "place_df.to_csv(r'gibb_places.csv')\n",
    "\n",
    "# Now bring this into GoogleMaps to see if it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDzb2M_qraPN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ5WyQts6W6_"
   },
   "source": [
    "### Pause for discussion!\n",
    "\n",
    "Why might some of the places you expect to appear on the map be missing?\n",
    "\n",
    "In your groups, experiment with different gazetteers and/or different time frames in the function `search_peripleo()`.\n",
    "\n",
    "Try mapping your new data. What changed? Why do you think you did or did not get different results? Share with the class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUOmLBEJ7NNh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gibbon_place_names_in_class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
